{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMsAtkflqjH4wucUgOo+Dyl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jennif8r/NLP/blob/main/Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wllOpltc0sVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "40ebcb78-cc23-4198-f5a1-2d543a15eb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==3.2.0\n",
            "  Downloading spacy-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (3.0.9)\n",
            "Collecting thinc<8.1.0,>=8.0.12 (from spacy==3.2.0)\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (0.7.11)\n",
            "Collecting wasabi<1.1.0,>=0.8.1 (from spacy==3.2.0)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (2.0.10)\n",
            "Collecting typer<0.5.0,>=0.3.0 (from spacy==3.2.0)\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Collecting pathy>=0.3.5 (from spacy==3.2.0)\n",
            "  Downloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (4.66.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (2.31.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy==3.2.0)\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.0) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.2.0) (1.2.0)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from pathy>=0.3.5->spacy==3.2.0)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy==3.2.0)\n",
            "  Downloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy==3.2.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2024.6.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.2.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.2.0) (1.2.0)\n",
            "Installing collected packages: wasabi, typer, smart-open, pydantic, pathlib-abc, thinc, pathy, spacy\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.3\n",
            "    Uninstalling wasabi-1.1.3:\n",
            "      Successfully uninstalled wasabi-1.1.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.12.3\n",
            "    Uninstalling typer-0.12.3:\n",
            "      Successfully uninstalled typer-0.12.3\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.0.4\n",
            "    Uninstalling smart-open-7.0.4:\n",
            "      Successfully uninstalled smart-open-7.0.4\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.0\n",
            "    Uninstalling pydantic-2.8.0:\n",
            "      Successfully uninstalled pydantic-2.8.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.2.0 which is incompatible.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pathlib-abc-0.1.1 pathy-0.11.0 pydantic-1.8.2 smart-open-6.4.0 spacy-3.2.0 thinc-8.0.17 typer-0.4.2 wasabi-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy==3.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixando a biblioteca do modelo pré-treinado em portugues:"
      ],
      "metadata": {
        "id": "iqaOiVI_Ckrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download 'pt_core_news_lg'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w_pUxaNmBoiE",
        "outputId": "7deb442f-2323-49c2-aec4-4b38514c2506"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-08 11:28:21.469291: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-08 11:28:21.469357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-08 11:28:21.472710: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-08 11:28:21.490875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-08 11:28:23.696722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.2.0/pt_core_news_lg-3.2.0-py3-none-any.whl#egg=pt_core_news_lg==3.2.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pt-core-news-lg==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.2.0/pt_core_news_lg-3.2.0-py3-none-any.whl (577.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.4/577.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-lg==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (0.7.11)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (0.11.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (4.66.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (1.2.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (6.4.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (2024.6.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.0->pt-core-news-lg==3.2.0) (1.2.0)\n",
            "Installing collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "bONb4uXfZf0B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('pt_core_news_lg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SnBmXlBZoKG",
        "outputId": "097391e5-993b-4c40-d7f9-aafd3c8a9498"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:431.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(nlp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "petBkgwFZzta",
        "outputId": "0c9850f5-9c65-4ced-cdb9-565762f28af0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.lang.pt.Portuguese'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vamos verificar o que tem no objeto:"
      ],
      "metadata": {
        "id": "VennVhZjaIb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1fW8hNeZ7TZ",
        "outputId": "4dfddd41-8869-4d01-d2e0-e14b1627267f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ao invez de subirmos um documento iremo criar e digitar seu dados dentro dele:\n",
        "\n",
        "documento = nlp(\"As ações da Magazine Luiza S.A.,Franca, Brasil, acumularam baixa de 70% ao ano. Assim já devolveram todos os ganhos do período da pandemia.\")\n",
        "#documento = nlp(\"alegações extraordinárias requerem evidências extraordinárias.carl sagan: 'https://carlsangan.com'\")"
      ],
      "metadata": {
        "id": "UaqEKsRFaSKC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documento.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgNYC5YIa1jU",
        "outputId": "fca8538b-49c2-4b3d-d26c-8725116ecae0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "381"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(documento))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqP3qzbBa5YL",
        "outputId": "6392bf41-b7b1-4866-d91d-98331a566ccd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Produção de Tokens"
      ],
      "metadata": {
        "id": "eDqLiVgxfn_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "Processo de separar a sentença em suas partes: palavras, pontos, símbolos.\n",
        "\n",
        "![Exemplo](/Imagem/Tokenization.png)"
      ],
      "metadata": {
        "id": "1Yx6i-wBf7e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pegar o nosso documento e fazer a tokenização dela.\n",
        "for token in documento:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lVDp-pvbFrP",
        "outputId": "7c683296-b461-42ea-85c9-7e416a081312"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As\n",
            "ações\n",
            "da\n",
            "Magazine\n",
            "Luiza\n",
            "S.A.,Franca\n",
            ",\n",
            "Brasil\n",
            ",\n",
            "acumularam\n",
            "baixa\n",
            "de\n",
            "70\n",
            "%\n",
            "ao\n",
            "ano\n",
            ".\n",
            "Assim\n",
            "já\n",
            "devolveram\n",
            "todos\n",
            "os\n",
            "ganhos\n",
            "do\n",
            "período\n",
            "da\n",
            "pandemia\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documento))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqMZhdQGG-6a",
        "outputId": "ba04d5d6-0689-4970-9dc3-171d4345e936"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identificar quais tokens é alfanumerico,stop word..."
      ],
      "metadata": {
        "id": "NAvxcyYKH4JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token: \", [token.text for token in documento])\n",
        "print(\"Stop word: \", [token.is_stop for token in documento])\n",
        "print(\"Alfanumerico: \", [token.is_alpha for token in documento])\n",
        "print(\"Maísculo: \", [token.is_upper for token in documento])\n",
        "print(\"Pontuação: \", [token.is_punct for token in documento])\n",
        "print(\"Número: \", [token.like_num for token in documento])\n",
        "print(\"Setença Inicial: \", [token.is_sent_start for token in documento])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_4DZXRvHDWq",
        "outputId": "9f7c7f87-6dfc-4efd-b2e5-2355e0c382d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token:  ['As', 'ações', 'da', 'Magazine', 'Luiza', 'S.A.,Franca', ',', 'Brasil', ',', 'acumularam', 'baixa', 'de', '70', '%', 'ao', 'ano', '.', 'Assim', 'já', 'devolveram', 'todos', 'os', 'ganhos', 'do', 'período', 'da', 'pandemia', '.']\n",
            "Stop word:  [True, False, True, False, False, False, False, False, False, False, False, True, False, False, True, False, False, True, True, False, True, True, False, True, False, True, False, False]\n",
            "Alfanumerico:  [True, True, True, True, True, False, False, True, False, True, True, True, False, False, True, True, False, True, True, True, True, True, True, True, True, True, True, False]\n",
            "Maísculo:  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
            "Pontuação:  [False, False, False, False, False, False, True, False, True, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True]\n",
            "Número:  [False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
            "Setença Inicial:  [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mostrar o formato das palavras\n",
        "print(\"Tokens: \", [token.text for token in documento])\n",
        "print(\"Formato: \", [token.shape_ for token in documento])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd6Nd3ccH_ud",
        "outputId": "debf6cf8-1a78-457b-fc5e-5540fb96c113"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['As', 'ações', 'da', 'Magazine', 'Luiza', 'S.A.,Franca', ',', 'Brasil', ',', 'acumularam', 'baixa', 'de', '70', '%', 'ao', 'ano', '.', 'Assim', 'já', 'devolveram', 'todos', 'os', 'ganhos', 'do', 'período', 'da', 'pandemia', '.']\n",
            "Formato:  ['Xx', 'xxxx', 'xx', 'Xxxxx', 'Xxxxx', 'X.X.,Xxxxx', ',', 'Xxxxx', ',', 'xxxx', 'xxxx', 'xx', 'dd', '%', 'xx', 'xxx', '.', 'Xxxxx', 'xx', 'xxxx', 'xxxx', 'xx', 'xxxx', 'xx', 'xxxx', 'xx', 'xxxx', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in documento:\n",
        "  if token.like_num:\n",
        "    print(\"Número encontrado: \", token.text)\n",
        "  if token.is_punct:\n",
        "    print(\"Pontuação encontrada: \", token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gniP4yYeIPUQ",
        "outputId": "3d7045d0-c0f6-4577-8697-edb296a6d8b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pontuação encontrada:  ,\n",
            "Pontuação encontrada:  ,\n",
            "Número encontrado:  70\n",
            "Pontuação encontrada:  %\n",
            "Pontuação encontrada:  .\n",
            "Pontuação encontrada:  .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part-Of-Speech\n",
        "Adiciona tags a cada token, como por exemplo, se é verbo, substantivo, etc.\n",
        "\n",
        "![Exemplo](/Imagem/PartsOfSpeechTaggong.png)"
      ],
      "metadata": {
        "id": "DpsbIXLQJFsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qhapb78JLAQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YtFlI-mHIvSI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}